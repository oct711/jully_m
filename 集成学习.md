# 集成学习

许多算法是可用的:决策树，神经网络，支持向量机
为了更好地解决特定的机器学习问题，策略性地生成和组合多个模型的过程。
动机：提高单个模型的性能；减少错误选择不良模型的可能性。
[![zBQB8g.png](https://s1.ax1x.com/2022/12/02/zBQB8g.png)](https://imgse.com/i/zBQB8g)
[![zB3eDs.png](https://s1.ax1x.com/2022/12/02/zB3eDs.png)](https://imgse.com/i/zB3eDs)
如何组合分类器？
投票
权重

The key to the success of ensemble learning
	Need to correct the errors made by other classifiers.
	Does not work if all models are identical.
Different Learning Algorithms
	DT, SVM, NN, KNN …
Different Training Processes
	Different Parameters
	Different Training Sets
	Different Feature Sets
Weak Learners
	Easy to create different decision boundaries.

有放回的采样
[![zB8ldI.md.png](https://s1.ax1x.com/2022/12/02/zB8ldI.md.png)](https://imgse.com/i/zB8ldI)
## Bagging
[![zB8GJf.md.png](https://s1.ax1x.com/2022/12/02/zB8GJf.md.png)](https://imgse.com/i/zB8GJf)
少数服从多数

实例：
随机森林

## Boosting

Stacking
[![zB8cSU.md.png](https://s1.ax1x.com/2022/12/02/zB8cSU.md.png)](https://imgse.com/i/zB8cSU)
Boosting
[![zB8q6e.md.png](https://s1.ax1x.com/2022/12/02/zB8q6e.md.png)](https://imgse.com/i/zB8q6e)[![zBGi6g.md.png](https://s1.ax1x.com/2022/12/02/zBGi6g.md.png)](https://imgse.com/i/zBGi6g)
## AdaBoost
AdaBoost是Adaptive Boosting（自适应增强）的缩写，它的自适应在于：被前一个基本分类器误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或预先指定的最大迭代次数再确定最后的强分类器。
[![zBJk8K.md.png](https://s1.ax1x.com/2022/12/02/zBJk8K.md.png)](https://imgse.com/i/zBJk8K)
## RegionBoost

AdaBoost为模型分配固定的权重。 然而，不同的模型强调不同的地区。模型的权重应该依赖于输入。给定一个输入，只调用适当的模型。为每个模型训练一个能力预测器。估计模型是否可能做出正确的决定。用这个信息作为权重。

[![zBJ18f.md.png](https://s1.ax1x.com/2022/12/02/zBJ18f.md.png)](https://imgse.com/i/zBJ18f)
[![zBJYrQ.png](https://s1.ax1x.com/2022/12/02/zBJYrQ.png)](https://imgse.com/i/zBJYrQ)

AdaBoost对错误率的训练可以达到0的，但对于未知数据，其效果并不如ReginBoost

[![zBJtbj.png](https://s1.ax1x.com/2022/12/02/zBJtbj.png)](https://imgse.com/i/zBJtbj)

集成学习的两种主要形式：
串行；并行
集成模型的不同方式：
平均数；
投票；
权重
