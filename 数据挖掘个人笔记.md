# 数据预处理

## 数据清洗

- 为什么进行数据清洗，数据在清洗之前是什么样子的

  数据不总是可用的：示例的一个或多个属性值可能具有空值；并不是所有的数据挖掘算法都可以处理空值；造成重大麻烦

  数据缺失的原因：设备故障；数据不被提供；不适用（N/A）

  数据缺失的类别：完全随机确实；有条件的缺失；

- 如何处理缺失数据

  1.忽略缺失数据：删除缺失值的示例或属性；直接删除

  2.手动添加缺失数据：重新采集；推测或猜测缺失值；

  3.随机生成缺失数据：根据全球平均数据；平均数或中位数；

  
- 离群点

  相对距离值越大，是离群点的可能性就越大
  [![zkgXVI.png](https://s1.ax1x.com/2022/11/14/zkgXVI.png)](https://imgse.com/i/zkgXVI)
  [![zkgzPf.png](https://s1.ax1x.com/2022/11/14/zkgzPf.png)](https://imgse.com/i/zkgzPf)
  

## 数据类型转换与采样

- 转换

  将数据转换为高维进行表达

- 采样

  不平衡数据集
  [![zk2uzF.png](https://s1.ax1x.com/2022/11/14/zk2uzF.png)](https://imgse.com/i/zk2uzF)
  向上采样

[![zk2lL9.png](https://s1.ax1x.com/2022/11/14/zk2lL9.png)](https://imgse.com/i/zk2lL9)
  边缘采样

  [![zk2NRO.png](https://s1.ax1x.com/2022/11/14/zk2NRO.png)](https://imgse.com/i/zk2NRO)

  

## 数据描述与可视化

- 标准化

  Min-max normalization（用于有明确上下界）

[![zk2DeA.png](https://s1.ax1x.com/2022/11/14/zk2DeA.png)](https://imgse.com/i/zk2DeA)

  Z-score normalization 

[![zk2rdI.png](https://s1.ax1x.com/2022/11/14/zk2rdI.png)](https://imgse.com/i/zk2rdI)

  

- 描述

[![zk2gW8.png](https://s1.ax1x.com/2022/11/14/zk2gW8.png)](https://imgse.com/i/zk2gW8)
[![zk25es.png](https://s1.ax1x.com/2022/11/14/zk25es.png)](https://imgse.com/i/zk25es)
  r>0:AB正相关

  r<0:AB负相关

  r=0:AB不是线性相关

- 可视化

  

## 特征选择

- 熵（数值越大，系统不确定性越高）

[![zk2ooq.png](https://s1.ax1x.com/2022/11/14/zk2ooq.png)](https://imgse.com/i/zk2ooq)

## 主成分分析

- 坐标旋转

  PCA：数据降维（无监督）

  PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

## 线性判别分析

PCA不适用于分类问题

- LDA（有监督）

  LDA算法的思路同PCA一致，即通过某种线性投影，将原本高维空间中的一些数据，映射到更低维度的空间中，但LDA算法要求投影后的数据满足：1.同类别的数据之间尽可能地接近。2.不同类别的数据之间尽可能地远离。

  

  

# 从贝叶斯到决策树

分类问题

有无标签、打标签

有监督的学习和无监督的学习的根本区别在于：学习样本是否需要人工标记

## 贝叶斯

鱼分类、射击例子、癌症问题、头疼流感问题

[![zkgRbR.png](https://s1.ax1x.com/2022/11/14/zkgRbR.png)](https://imgse.com/i/zkgRbR)



- 朴素贝叶斯

  条件独立

  朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法 。

  原理：根据特征的先验概率（训练样本分析得到的概率），利用贝叶斯公式计算出其后验概率（要分类对象特征的条件概率），选择概率值最大的类作为该特征所属的类。

  - 拉普拉斯平滑

    **为了避免其他属性携带的信息被其他未出现过的属性值“抹去”，在估计概率值时通常要进行平滑。具体的说，令N表示训练集D中的类别数，Ni表示第i个属性可能的取值数，则：**

    [![zkgo8O.png](https://s1.ax1x.com/2022/11/14/zkgo8O.png)](https://imgse.com/i/zkgo8O)
    [![zkgT2D.png](https://s1.ax1x.com/2022/11/14/zkgT2D.png)](https://imgse.com/i/zkgT2D)

    

- 决策树

  纯度：**目标变量的分歧越小纯度越高**。

  信息熵：**对整个概率分布中的不确定性总量进行量化。**

  [![zkgqrd.png](https://s1.ax1x.com/2022/11/14/zkgqrd.png)](https://imgse.com/i/zkgqrd)

  当数据的不确定性越大时，信息熵也就越大

  信息增益

  优势：可以提取规则

  - ID3

  - 过拟合（overfitting）

    为了得到一致假设而使假设变得过度复杂，**过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差**。

    如何防止过拟合：数据集扩增、改进模型、正则化、剪枝

    剪枝：预剪枝策略、后剪枝策略

    预剪枝：**边构造边剪枝。**预剪枝指在每个结点划分前就进行估计，考虑结点对于整体划分效果是否有提升。如果当前结点的划分不能对决策树泛化性能有提高吗，则停止划分并将当前结点标记为叶结点。

    后剪枝：**构造完再剪枝。**后剪枝则是先从训练集中生成一棵完整的决策树，然后自底向上的考察每一个非叶结点，和预剪枝相反，如果将该结点对应的子树替换为叶结点能给决策树带来性能上的提升，则将该子树替换为叶结点。


# 神经网络

神经元，又被称为感知机
一个神经元包含很多输入+激活函数+输出

[![zkgJgg.png](https://s1.ax1x.com/2022/11/14/zkgJgg.png)](https://imgse.com/i/zkgJgg)

构造神经元要有一个w0，使其能在超平面之间来回跳转，控制判决平面到远点的距离

- 梯度下降

  批处理
  [![zkgUDs.png](https://s1.ax1x.com/2022/11/14/zkgUDs.png)](https://imgse.com/i/zkgUDs)
  [![zkgwEq.png](https://s1.ax1x.com/2022/11/14/zkgwEq.png)](https://imgse.com/i/zkgwEq)
  误差对w求偏导，如果结果是正的，即随着w的增加，误差增加。要减小误差就要减小权重，即前面加负号

  学习率控制学习的幅度。

[![zk2bWT.png](https://s1.ax1x.com/2022/11/14/zk2bWT.png)](https://imgse.com/i/zk2bWT)

- Stochastic Learning

  立即修改

  [![zkg0U0.png](https://s1.ax1x.com/2022/11/14/zkg0U0.png)](https://imgse.com/i/zkg0U0)

- 多层感知机

[![zkgB5V.png](https://s1.ax1x.com/2022/11/14/zkgB5V.png)](https://imgse.com/i/zkgB5V)

  多层感知机在单层神经网络的基础上引入了一到多个隐藏层，位于输入和输出层之间

  在解决线性不可分的问题时，将其映射到隐含层变为线性可分的问题

  - 激活函数 sigmoid

[![zk2Xy4.png](https://s1.ax1x.com/2022/11/14/zk2Xy4.png)](https://imgse.com/i/zk2Xy4)

  ​       使其限制输入上下限，解析度好

  

- BP

[![zkggKJ.png](https://s1.ax1x.com/2022/11/14/zkggKJ.png)](https://imgse.com/i/zkggKJ)

其他网络

Elman Network、Hopfield Network

# 聚类

无监督

聚类分析：就是寻找簇的过程，簇只是区分类别，而不是具有标签性质

应用：市场营销、地震、社交网络、图像分割

聚类分析过程：

数据预处理（特征选择、数据降维等）

选择度量

分组（如果聚类结果好就直接输出，反之，则反馈到前面，执行一二步）

[![zkgejH.png](https://s1.ax1x.com/2022/11/14/zkgejH.png)](https://imgse.com/i/zkgejH)

一般性要求：能够发现具有任意形状的集群、要求能够处理噪声和异常值等

## K-Means

- 算法过程：

​		确定K的值

​		随机选择K个簇中心。

​		【每个数据点都分配给它最近的质心。

​		使用的每个集群更新每一个重心。】

​		重复这个过程，直到没有新的任务。

​		返回K形心。

- 优点：

​		优点简单，适用于规则的不相交集群。

​		收敛较快。

​		相对高效和可扩展O(t-k-n)t:迭代;K:质心数;N:数据点个数

- 缺点：

​		K值无法直接给定

​		可能会收敛到局部最优解

​		对噪声和离群点比较敏感

## Sequential Leader Clustering

一个非常高效的聚类算法。

不需要提前指定K。

选择集群阈值。对于每一个新的数据点:计算新数据点到每个集群中心之间的距离。如果最小距离小于所选阈值，则将新的数据点分配到相应的集群，并重新计算集群中心。否则，创建一个以新数据点为中心的新集群。

聚类结果可能受到数据点序列的影响。

## 高斯混合模型

[![zkgKHI.png](https://s1.ax1x.com/2022/11/14/zkgKHI.png)](https://imgse.com/i/zkgKHI)
[![zkg14f.png](https://s1.ax1x.com/2022/11/14/zkg14f.png)](https://imgse.com/i/zkg14f)
EM算法

## 基于密度聚类

生成任意形状的集群；不需要提前设置K值；有点类似于人类的视觉。

常用方法：DBSCAN

将数据划分成三类：

- Core Point：核心点
- Border Point：边缘点（在核心点周围）
- Noise Point：噪点（需要消除）

## 基于层次聚类

生成一组嵌套的树状集群。

可视化为树状图；聚类是通过切割到所需的水平来获得的；不需要提前指定K。

- Agglomerative Methods

​		将每个数据点分配到一个集群中

​		【计算接近矩阵

​		合并最近的一对集群

​		重复这个过程，直到只剩下一个集群】



# 关联规则

## 项集与规则

Items
- Bread, Milk, Chocolate, Butter …
Transaction (Basket)
- A non-empty subset of all items
itemset
- A set of items
A transaction is a set of items: T={ia, ib,…,it}
T is a subset of I where I is the set of all possible items.
The dataset D contains a set of transactions.

关联规则形式：
[![zMBFts.png](https://s1.ax1x.com/2022/11/20/zMBFts.png)](https://imgse.com/i/zMBFts)

[![zMBV10.png](https://s1.ax1x.com/2022/11/20/zMBV10.png)](https://imgse.com/i/zMBV10)

## 支持度与置信度

[![zMB2DS.png](https://s1.ax1x.com/2022/11/20/zMB2DS.png)](https://imgse.com/i/zMB2DS)
例：
[![zMBv59.png](https://s1.ax1x.com/2022/11/20/zMBv59.png)](https://imgse.com/i/zMBv59)
买牛奶的人会去买面包这条规则更强

最小支持度σ
最低信任度Φ
频繁项集的支持度大于σ。
强规则是频繁出现的规则，其置信度高于Φ。
Given I, D, σ and Φ, to find all strong rules in the form of X->Y.

[![zMrlS1.png](https://s1.ax1x.com/2022/11/20/zMrlS1.png)](https://imgse.com/i/zMrlS1)
## 误区

1.一个规则很强并不代表它有意义
2.两个商品出现概率差别非常大
3.Association ≠ Causality

## Apriori

关键点

频繁项集的子集必须是频繁的。
{Milk, Bread, Coke} is frequent  -> {Milk, Coke} is frequent
任何不频繁项集的超集都不能频繁。
{Battery} is infrequent ->{Milk, Battery} is infrequent



# 集成学习

许多算法是可用的:决策树，神经网络，支持向量机
为了更好地解决特定的机器学习问题，策略性地生成和组合多个模型的过程。
动机：提高单个模型的性能；减少错误选择不良模型的可能性。
[![zBQB8g.png](https://s1.ax1x.com/2022/12/02/zBQB8g.png)](https://imgse.com/i/zBQB8g)
[![zB3eDs.png](https://s1.ax1x.com/2022/12/02/zB3eDs.png)](https://imgse.com/i/zB3eDs)
如何组合分类器？
投票
权重

The key to the success of ensemble learning
	Need to correct the errors made by other classifiers.
	Does not work if all models are identical.
Different Learning Algorithms
	DT, SVM, NN, KNN …
Different Training Processes
	Different Parameters
	Different Training Sets
	Different Feature Sets
Weak Learners
	Easy to create different decision boundaries.

有放回的采样
[![zB8ldI.md.png](https://s1.ax1x.com/2022/12/02/zB8ldI.md.png)](https://imgse.com/i/zB8ldI)
## Bagging
[![zB8GJf.md.png](https://s1.ax1x.com/2022/12/02/zB8GJf.md.png)](https://imgse.com/i/zB8GJf)
少数服从多数

实例：
随机森林

## Boosting

Stacking
[![zB8cSU.md.png](https://s1.ax1x.com/2022/12/02/zB8cSU.md.png)](https://imgse.com/i/zB8cSU)
Boosting
[![zB8q6e.md.png](https://s1.ax1x.com/2022/12/02/zB8q6e.md.png)](https://imgse.com/i/zB8q6e)[![zBGi6g.md.png](https://s1.ax1x.com/2022/12/02/zBGi6g.md.png)](https://imgse.com/i/zBGi6g)
## AdaBoost
AdaBoost是Adaptive Boosting（自适应增强）的缩写，它的自适应在于：被前一个基本分类器误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或预先指定的最大迭代次数再确定最后的强分类器。
[![zBJk8K.md.png](https://s1.ax1x.com/2022/12/02/zBJk8K.md.png)](https://imgse.com/i/zBJk8K)
## RegionBoost

AdaBoost为模型分配固定的权重。 然而，不同的模型强调不同的地区。模型的权重应该依赖于输入。给定一个输入，只调用适当的模型。为每个模型训练一个能力预测器。估计模型是否可能做出正确的决定。用这个信息作为权重。

[![zBJ18f.md.png](https://s1.ax1x.com/2022/12/02/zBJ18f.md.png)](https://imgse.com/i/zBJ18f)
[![zBJYrQ.png](https://s1.ax1x.com/2022/12/02/zBJYrQ.png)](https://imgse.com/i/zBJYrQ)

AdaBoost对错误率的训练可以达到0的，但对于未知数据，其效果并不如ReginBoost

[![zBJtbj.png](https://s1.ax1x.com/2022/12/02/zBJtbj.png)](https://imgse.com/i/zBJtbj)

集成学习的两种主要形式：
串行；并行
集成模型的不同方式：
平均数；
投票；
权重


# 进化计算

例子：
投资优化
旅行推销员问题
背包问题
装包分配问题

## GA遗传算法
一些基本组成
1.表示：
	如何进行编码？
	二元问题等
	[![zBtljS.md.png](https://s1.ax1x.com/2022/12/02/zBtljS.md.png)](https://imgse.com/i/zBtljS)
2.遗传算子：
	杂交
		在两条染色体之间交换遗传物质。
		1.一点杂交
			选择一个位置，后面的进行交换，前面的保持不变
			[![zBtW36.md.png](https://s1.ax1x.com/2022/12/02/zBtW36.md.png)](https://imgse.com/i/zBtW36)
		2.二点杂交
			对两个位置之间的进行互换，其他的保持不变
			[![zBtIDe.md.png](https://s1.ax1x.com/2022/12/02/zBtIDe.md.png)](https://imgse.com/i/zBtIDe)
		3.各个点进行杂交
			[![zBtLCt.md.png](https://s1.ax1x.com/2022/12/02/zBtLCt.md.png)](https://imgse.com/i/zBtLCt)
	基因突变
		在选定的位置随机修改基因值。
		[![zBNk80.md.png](https://s1.ax1x.com/2022/12/02/zBNk80.md.png)](https://imgse.com/i/zBNk80)
3.选择策略
	哪些染色体应该参与繁殖?
	哪些后代能够存活下来?
	策略1：
		轮盘赌轮的选择
	策略2：
		等级的选择
	策略3：
		通过决斗选择
	策略4：
		精英主义选择，后代选择

框架
[![zBNUVH.md.png](https://s1.ax1x.com/2022/12/02/zBNUVH.md.png)](https://imgse.com/i/zBNUVH)
参数
[![zBN0PI.md.png](https://s1.ax1x.com/2022/12/02/zBN0PI.md.png)](https://imgse.com/i/zBN0PI)
遗传算法应用：
特征选择
聚类
参数控制
约束处理
多目标优化

## GP遗传编程

GA与GP
[![zBd32q.md.png](https://s1.ax1x.com/2022/12/02/zBd32q.md.png)](https://imgse.com/i/zBd32q)


# 推荐

基于内容的过滤
	着重于物品的特性。
	推荐与用户过去喜欢的相似的项目。
协同过滤
	根据用户与其他用户的相似度来预测用户会喜欢什么。
	类似于询问朋友的意见。
	不依赖于机器分析的内容。

音乐推荐
[![zRL8OO.md.png](https://s1.ax1x.com/2022/12/09/zRL8OO.md.png)](https://imgse.com/i/zRL8OO)

## Tf-idf

Term Frequency (TF)
词出现的频率
[![zROG3q.png](https://s1.ax1x.com/2022/12/09/zROG3q.png)](https://imgse.com/i/zROG3q)
Inverse Document Frequency (IDF)
对于该词在本文档以外的文档出现的频率越低越好
[![zROwE4.png](https://s1.ax1x.com/2022/12/09/zROwE4.png)](https://imgse.com/i/zROwE4)

## Vector Space Model

将文档变成向量类型

[![zRX8de.md.png](https://s1.ax1x.com/2022/12/09/zRX8de.md.png)](https://imgse.com/i/zRX8de)

存在的问题：
	同义：不同的词，相同的意思
	一词多义

## Latent Semantic Analysis

隐含语义分析

[![zRjt7F.md.png](https://s1.ax1x.com/2022/12/09/zRjt7F.md.png)](https://imgse.com/i/zRjt7F)

## pagerank

PageRank算法的基本想法是在有向图上定义一个随机游走模型，即一阶马尔可夫链，描述随机游走者沿着有向图随机访问各个结点的行为。在一定条件下，极限情况访问每个结点的概率收敛到平稳分布，这时各个结点的平稳概率值就是其PageRank值，表示结点的重要度。PageRank 是递归定义的，PageRank 的计算可以通过迭代算法进行。

[![zRzqAO.md.png](https://s1.ax1x.com/2022/12/09/zRzqAO.md.png)](https://imgse.com/i/zRzqAO)

## 协同过滤

核心理念:
	人们从志同道合的人那里得到最好的推荐。
工作流程:
	创建一个评级或购买矩阵。
	通过匹配他们的评分来找到相似的人。
	推荐同类人评价高的物品。

基于用户和基于物品的协同过滤

冷启动问题：一件新商品或者新客户如何进行推荐

### 基于用户的协同过滤
对于用户 A，根据用户的历史偏好，计算得到一个邻居 - 用户 C，然后将用户 C 喜欢的物品 D推荐给用户 A。
[![zWSXZV.png](https://s1.ax1x.com/2022/12/09/zWSXZV.png)](https://imgse.com/i/zWSXZV)
[![zWpAZ6.md.png](https://s1.ax1x.com/2022/12/09/zWpAZ6.md.png)](https://imgse.com/i/zWpAZ6)
[![zWpMQA.md.png](https://s1.ax1x.com/2022/12/09/zWpMQA.md.png)](https://imgse.com/i/zWpMQA)
### 基于物品的协同过滤

基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。计算物品的相似度矩阵。

[![zWCZgH.png](https://s1.ax1x.com/2022/12/09/zWCZgH.png)](https://imgse.com/i/zWCZgH)
[![zWCevd.png](https://s1.ax1x.com/2022/12/09/zWCevd.png)](https://imgse.com/i/zWCevd)

### Model-Based CF

[![zWkpRO.md.png](https://s1.ax1x.com/2022/12/09/zWkpRO.md.png)](https://imgse.com/i/zWkpRO)
[![zWkisH.md.png](https://s1.ax1x.com/2022/12/09/zWkisH.md.png)](https://imgse.com/i/zWkisH)