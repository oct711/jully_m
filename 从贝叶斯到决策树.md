# 从贝叶斯到决策树

分类问题

有无标签、打标签

有监督的学习和无监督的学习的根本区别在于：学习样本是否需要人工标记

## 贝叶斯

鱼分类、射击例子、癌症问题、头疼流感问题

[![zkgRbR.png](https://s1.ax1x.com/2022/11/14/zkgRbR.png)](https://imgse.com/i/zkgRbR)



- 朴素贝叶斯

  条件独立

  朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法 。

  原理：根据特征的先验概率（训练样本分析得到的概率），利用贝叶斯公式计算出其后验概率（要分类对象特征的条件概率），选择概率值最大的类作为该特征所属的类。

  - 拉普拉斯平滑

    **为了避免其他属性携带的信息被其他未出现过的属性值“抹去”，在估计概率值时通常要进行平滑。具体的说，令N表示训练集D中的类别数，Ni表示第i个属性可能的取值数，则：**

    [![zkgo8O.png](https://s1.ax1x.com/2022/11/14/zkgo8O.png)](https://imgse.com/i/zkgo8O)
    [![zkgT2D.png](https://s1.ax1x.com/2022/11/14/zkgT2D.png)](https://imgse.com/i/zkgT2D)

    

- 决策树

  纯度：**目标变量的分歧越小纯度越高**。

  信息熵：**对整个概率分布中的不确定性总量进行量化。**

  [![zkgqrd.png](https://s1.ax1x.com/2022/11/14/zkgqrd.png)](https://imgse.com/i/zkgqrd)

  当数据的不确定性越大时，信息熵也就越大

  信息增益

  优势：可以提取规则

  - ID3

  - 过拟合（overfitting）

    为了得到一致假设而使假设变得过度复杂，**过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差**。

    如何防止过拟合：数据集扩增、改进模型、正则化、剪枝

    剪枝：预剪枝策略、后剪枝策略

    预剪枝：**边构造边剪枝。**预剪枝指在每个结点划分前就进行估计，考虑结点对于整体划分效果是否有提升。如果当前结点的划分不能对决策树泛化性能有提高吗，则停止划分并将当前结点标记为叶结点。

    后剪枝：**构造完再剪枝。**后剪枝则是先从训练集中生成一棵完整的决策树，然后自底向上的考察每一个非叶结点，和预剪枝相反，如果将该结点对应的子树替换为叶结点能给决策树带来性能上的提升，则将该子树替换为叶结点。
